<p><img src="https://chloenelkin.files.wordpress.com/2012/12/cat-7-mondrian-composition-c-no-iii-with-red-yellow-and-blue-1935_stretchcmyk-no-6-in-photo-sheet1.jpg" alt="placeholder" />
<em>Composition C by Piet Mondiran.</em></p>

<h2 id="motivation">Motivation</h2>
<p>The confusion matrix plays a huge role in evaluating performance of a statistical model (or a diagnostic device). Below is a self-explanatory example of a binary confusion matrix.</p>

<p><img src="https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png" alt="placeholder" /></p>

<p>For example, assume someone developed a device that detects ADHD. He then conducted some clinical tests to justify the validity of the device. It is given that, of all the 1000 test subjects, 200 actually has ADHD and the rest are control. The device, oblivious of the actual distribution, identified 180 ADHD-positive individuals. Among these 180 subjects, 170 are “real” ADHD patients and the rest are controls misclassified by the device. In this case, we have 170 true positives (TPs), 180 - 170 = 10 false positives (FPs), 800 - 10 = 790 true negatives (TNs), and 200 - 170 = 30 false negatives (FNs).</p>

<h2 id="metrics">Metrics</h2>
<p>Natually, one would want as many TP and TN as possible relative to FP and FN. Therefore, scientists developed many derived values from the confusion matrix. Some of them are used interchangeably and are indeed the same thing. Some of them are used ambiguously but are in fact not the same. Here is a list of all the metrics derived from the confusion matrix and their usage:</p>

<ul>
  <li>
    <p><strong>Precision</strong> (with no alternative names so far) is <script type="math/tex">\frac{TP}{TP + FP}</script> in other words how many samples identified as positive by the model/device are actually positive. It is a measure of how well the model avoids classifying negative samples as positive.</p>
  </li>
  <li>
    <p><strong>Recall</strong>, also called <strong>Sensitivity</strong> or <strong>True Positive Rate</strong>, is <script type="math/tex">\frac{TP}{TP + FN}</script>. Since the sum of TP and FN is the total number of positive samples, recall measures how well the model “picks out” the positive samples, i.e. what proportion of all the positive samples are correctly identified.</p>
  </li>
  <li>
    <p><strong>Specificity</strong>, also called <strong>True Negative Rate</strong>, is <script type="math/tex">\frac{TN}{TN + FP}</script>. It is like recall but for negative samples. The sum of TN and FP is the total number of negative samples. Therefore, specificity measures what proportion of all the negative samples are correctly identified.</p>
  </li>
</ul>

<p>People usually say “precision &amp; recall” or “sensitivity &amp; specificity” as if they are intrinsically paired. Indeed, precision and recall are both used to compute another metric called F1-score. Higher F1-score indicates better accuracy because it is the harmonic mean of precision (<script type="math/tex">P</script>) and recall (<script type="math/tex">R</script>):</p>

<script type="math/tex; mode=display">F_{1} = 2\frac{PR}{P + R}</script>

<p>On the other hand, sensitivity (<script type="math/tex">TPR</script>) and 1 - specificity (<script type="math/tex">1 - TNR = FPR</script>, false positive rate) are the y- and x-axis for the ROC (receiver operating characteristic) curve respectively. There will probably be another article discussing the ROC in detail coming soon.</p>

<h2 id="final-notes">Final notes</h2>
<p>A multitide of statistical metrics have been invented to address the vagueness of the term “model accuracy”. Also, having high score for one metric does not necessarily imply the same for another. A model may have great sensitivity but poor precision because it simply identifies all samples as positive.</p>
