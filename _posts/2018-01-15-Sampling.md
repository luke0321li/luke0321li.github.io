---
title: Random sampling from a large reservoir
---
![placeholder](https://tinashealthyliving.com/wp-content/uploads/2017/07/Basmati-Rice.jpg)
*Photo source: https://tinashealthyliving.com/wp-content/uploads/2017/07/Basmati-Rice.jpg*

## Motivation
During quantitative or statistical research, sometimes one may attempt to randomly sample from a large reservoir of data. For example, in k-fold cross validation, $$ \frac{1}{k} $$ of the original data is randomly selected. 

Give the original dataset of size $$n$$ and a desired sample size $$m$$, a naive way of implementing a random sampling algorithm is to generate a random number $$r \in [0, n - 1]$$ and then put the $$r$$th item  into the sample; the process is repeated until there are $$m$$ items in the sample. However, this method does not work when dealing with streamed data (coming in indefinitely) because one does not have knowledge of $$n$$.     

This post explores efficient computational apporaches for random sampling that covers the case where size of the original dataset is not necessarily known.  

## Methods
### Approach 1
The first approach is to maintain a priority queue. Initially when no data is present the priority queue is empty. Then, A random number (usually between 0 and 1) is generated and assigned to each data item as a key. Traverse the dataset and push each item (paired with their random number key) into the priority queue, which sorts the items based on their keys, until the size of the queue reaches $$m$$. 

Now, generate the random key $$r_{m + i}$$ for the ($$m + i$$)th item as usual. Investigate the top element (item with largest key) in the priority queue: if its key is larger than $$r_{m + i}$$, remove it and push the ($$m + i$$)th item into the priority queue; otherwise, discard the ($$m + i$$)th item. The same is done for all the upcoming items; anyhow, the size of the priority queue will be maintained at $$m$$. This is equivalent to generating a random key for all items in a dataset and keeping the $$m$$ smallest ones. 

This method is easily understood. It requires $$O(m)$$ space for storing the sample and $$O(nlog(m))$$ time to process all the $$n$$ samples (because $$O(log(m))$$ time is required for pushing items into the priority queue).

*Can it be done in linear time?*
### Approach 2 
Yes, but in a slightly more convoluted way, as described in [this paper](http://www.cs.umd.edu/~samir/498/vitter.pdf). To achieve such linear time complexity, traverse the dataset and put the first $$m$$ items into an array of size $$m$$ (indexed from 1 to $$m$$, for the sake of convenience). For the ($$m + i$$)th item, generate a random number $$r_{m + i} \in [1, m + i]$$. If $$r_{m + i} \leq m$$, replace the ($$r_{m + i}$$)th item in the array with the ($$m + i$$)th item; otherwise discard the ($$m + i$$)th item. 

Now since accessing an array item at some known index takes constant time, this approach obviously needs only $$O(n)$$ time in general, with the same memory requirement as the previous approach. However, how does this method guarantee that every item has the same probability of being present in the sample? 

Let $$P(item_{k} \in Sample; n)$$ be the probability that after scanning $$n$$ items, the $$k$$th item in the original dataset is in the final sample. 
#### Case 1: $$k > m$$
If $$k > m$$, the first requisite is that the random number $$r_{k}$$ generated for the $$k$$th item should be smaller or equal to $$m$$. If so, the $$k$$th item can supercede the $$r_{k}$$th item in the sample. Since $$r_{k} \in [1, k]$$ there is a $$\frac{m}{k}$$ chance that $$r_{k} \leq m$$.

In addition, if the $$k$$th item is to remain in the sample until all $$n$$ items are scanned, every subsequent random number $$r_{k + i}$$ should not be equal to $$r_{k}$$; otherwise, the $$k$$th item which is now placed at the ($$r_{k}$$)th position in the array would be replaced. $$P(r_{k + i} =  r_{k}) = \frac{1}{k + i}$$ and it follows that, for all $$i \geq 1$$: 

$$P(r_{k + i} \neq r_{k}) = 1 - \frac{1}{k + i} = \frac{k + i - 1}{k + i}$$ 

Combining the two requisites above, $$P(item_{k} \in Sample; n)$$ is obtained as:

$$ \frac{m}{k}\prod_{i = 1}^{n - k}P(r_{k + i} \neq r_{k}) =  \frac{m}{k}\prod_{i = 1}^{n - k}\frac{k + i - 1}{k + i} = \frac{m}{n} $$

The same holds for all $$k > m$$.
#### Case 2: $$k \leq m$$
Now since the $$k$$th item is already in the sample, it needs to remain there until all items are scanned. All random numbers generated $$r_{m + i}$$ should not be equal to $$k$$. In other words, 

$$ P(item_{k} \in Sample; n) = \prod_{i = 1}^{n - m}P(r_{m + i} \neq k) = \prod_{i = 1}^{n - m}\frac{m + i - 1}{m + i} = \frac{m}{n} $$

A possible implementation in C++ is shown below:
{% highlight c++%}
vector<Object> rndSample(vector<Object> dataset, int m)
{
    vector<Object> output(m);
    for (int i = 0; i < dataset.size(); i++)
    {
        if (i <= m - 1)
            output[i] = dataset[i];
        else
        {
            int r = rand() % (i + 1);
            if (r <= m - 1)
                output[r] = dataset[i];
        } 
    }
    return output;
}
{% endhighlight %}

## Discussion
The two methods described above are the better known sampling algorithms. Their variants can also solve some additional problems:

* What if sampling with replacement (the same item is allowed to appear multiple times in the sample) is allowed?

* What if each sample has a "weight" (some predefined probability of being selected in the sample)?
